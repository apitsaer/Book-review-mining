{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.45.217.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remarks / TO DO\n",
    "# 2. add to pre-processing stemming\n",
    "# 3. also try Naive Bayes and SVM (on top of log regr). See https://towardsdatascience.com/multi-class-text-classification-with-pyspark-7d78d022ed35\n",
    "# see also: http://classes.ischool.syr.edu/ist718/content/unit09/lab-sentiment_analysis/\n",
    "# 6. VERY IMPORTANT: I think we should rather reduce the number of categories from 5 to let's say 3\n",
    "# the 3 categories would be bad (0 and 1 star), middle (3 star), good (4 and 5 stars)\n",
    "# this would allow to have more training instance per categories and anyway how can even a human differentiate a 1 from a 2 stars or a 4 from a 5 stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data loading and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with easy implemetation: only consider the content of the 2 fields review_title and review_text\n",
    "# concantenate them in one new field \"review_concat\"from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "filepath = 'data_processed/ExctractedData.json'\n",
    "# load JSON file\n",
    "s_df = spark.read.json(filepath)\n",
    "s_df.count()\n",
    "s_df = s_df.drop_duplicates(subset=['review_id'])\n",
    "pd_df = s_df.groupBy('review_id').count().toPandas().set_index(\"count\").sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R15DG6BI3K1I78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R1UU50BM0S4LPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R27KEMBTEQ4MHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R1HMP34XP1V9BE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R22I2JYOOXA3PP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            review_id\n",
       "count                \n",
       "1      R15DG6BI3K1I78\n",
       "1      R1UU50BM0S4LPY\n",
       "1      R27KEMBTEQ4MHI\n",
       "1      R1HMP34XP1V9BE\n",
       "1      R22I2JYOOXA3PP"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# control no duplicate\n",
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- book_title: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- review_score: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- review_title: string (nullable = true)\n",
      " |-- review_user: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- review_concat: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# concatenate review text and title in one field\n",
    "s_df = s_df.withColumn('review_concat',fn.concat(fn.col('review_title'),fn.lit(' '), fn.col('review_text')))\n",
    "# review_score is of type String ==> cast it from String to Integer\n",
    "s_df = s_df.withColumn(\"review_score\", s_df[\"review_score\"].cast(IntegerType()))\n",
    "#s_df = s_df.withColumn(\"book_id\", s_df[\"book_id\"].cast(IntegerType()))\n",
    "s_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of rows: 11573\n",
      "# of rows per class:\n",
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5| 9383|\n",
      "|           4| 1529|\n",
      "|           3|  346|\n",
      "|           2|  170|\n",
      "|           1|  145|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Total # of rows: ' + str(s_df.count()))\n",
    "print('# of rows per class:')\n",
    "s_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(book_id='0062678426', book_title='The Woman in the Window: A Novel', review_id='R15DG6BI3K1I78', review_score=5, review_text=\"Extraordinary on any & every level. Astonishing that it' s a debut novel. Transfixing.\", review_title='Although reviews are universally stellar, highly recommend one avoids reading them & any synopsis preplunging in.', review_user='Perel Soreh', timestamp=1556661613, review_concat=\"Although reviews are universally stellar, highly recommend one avoids reading them & any synopsis preplunging in. Extraordinary on any & every level. Astonishing that it' s a debut novel. Transfixing.\")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at first 5 star review\n",
    "s_df.where(fn.col('review_score') == 5).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(book_id='0062824619', book_title='Cemetery Road: A Novel', review_id='R1T4O9RXIKX7D9', review_score=1, review_text='I am a huge fan of Greg Isles, but Cemetery Road was a outline of the garbage that the publishers must insist on before they will publish your book.  Mr. Isles, you are better than this, and you disappointed us with Cemetery Road.  I am going back to your older books, which are far superior to your latest endeavor.  In closing, there are no grey areas like you are suggesting in your book.  It is either moral or immoral.  There is no in between.', review_title='Disappointed', review_user='Jeanette Grayeb-Mihal', timestamp=1554878526, review_concat='Disappointed I am a huge fan of Greg Isles, but Cemetery Road was a outline of the garbage that the publishers must insist on before they will publish your book.  Mr. Isles, you are better than this, and you disappointed us with Cemetery Road.  I am going back to your older books, which are far superior to your latest endeavor.  In closing, there are no grey areas like you are suggesting in your book.  It is either moral or immoral.  There is no in between.')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at 1 very bad review\n",
    "s_df.where(fn.col('review_score') == 1).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(review_concat='Disappointed I am a huge fan of Greg Isles, but Cemetery Road was a outline of the garbage that the publishers must insist on before they will publish your book.  Mr. Isles, you are better than this, and you disappointed us with Cemetery Road.  I am going back to your older books, which are far superior to your latest endeavor.  In closing, there are no grey areas like you are suggesting in your book.  It is either moral or immoral.  There is no in between.')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show ony review_concat field\n",
    "s_df.select('review_concat').where(fn.col('review_score') == 1).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import stop words to filter them out from the reviews\n",
    "import requests\n",
    "stop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()\n",
    "stop_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(tfidf=SparseVector(7909, {0: 0.9577, 7: 1.7476, 14: 2.1023, 42: 2.6845, 55: 2.8888, 138: 3.4816, 165: 3.6103, 201: 7.4795, 302: 4.3067, 462: 4.3459, 602: 4.8132, 1009: 10.2448, 1148: 5.1518, 1356: 5.5723, 1470: 5.5064, 2092: 12.7216, 2504: 6.0984, 2882: 6.1785, 3720: 6.5839, 3832: 6.7175, 4788: 6.7916, 6038: 7.1593, 6856: 7.2771, 7124: 7.4106, 7131: 7.4106, 7214: 15.1295}))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define processing 4 steps and execute them with a trsnformation pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1. Tokenizer, .setPattern(\"\\\\p{L}+\") means that it remove accent from words (check it has no impact on the smileys !!!)\n",
    "tokenizer = RegexTokenizer().setGaps(False)\\\n",
    "  .setPattern(\"\\\\p{L}+\")\\\n",
    "  .setInputCol(\"review_concat\")\\\n",
    "  .setOutputCol(\"words\")\n",
    "\n",
    "# 2. filter out stop words\n",
    "sw_filter = StopWordsRemover()\\\n",
    "  .setStopWords(stop_words)\\\n",
    "  .setCaseSensitive(False)\\\n",
    "  .setInputCol(\"words\")\\\n",
    "  .setOutputCol(\"filtered\")\n",
    "\n",
    "# 3. TF: TF vectorization + remove words that appear in 5 docs or less\n",
    "#  converts text documents to vectors of term counts\n",
    "cv = CountVectorizer(minTF=1., minDF=5., vocabSize=2**17)\\\n",
    "  .setInputCol(\"filtered\")\\\n",
    "  .setOutputCol(\"tf\")\n",
    "\n",
    "# 4. TF-IDF transform\n",
    "# The IDFModel takes feature vectors (generally created from HashingTF or CountVectorizer) and scales each column. \n",
    "# Intuitively, it down-weights columns which appear frequently in a corpus.\n",
    "idf = IDF().\\\n",
    "    setInputCol('tf').\\\n",
    "    setOutputCol('tfidf')\n",
    "\n",
    "# Create a pipelined transformer and fit it with full data set\n",
    "tfidf_pipeline = Pipeline(stages=[tokenizer, sw_filter, cv, idf]).fit(s_df)\n",
    "\n",
    "# Control execution of preprocessing pipeline by pre-processing the data\n",
    "s_df_transform = tfidf_pipeline.transform(s_df)\n",
    "s_df_transform.select('tfidf').where(fn.col('review_score') == 1).first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- book_title: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- review_score: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- review_title: string (nullable = true)\n",
      " |-- review_user: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- review_concat: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tf: vector (nullable = true)\n",
      " |-- tfidf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check schema of output of preprocessing pipeline \n",
    "s_df_transform.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model training\n",
    "## 3.1. Simple logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9191, 2382]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random split in train and test set with 80-20% proportions\n",
    "training_df, testing_df = s_df.randomSplit([0.8, 0.2], seed=42)\n",
    "[training_df.count(), testing_df.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add logistic regression to the previously defined pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression().\\\n",
    "    setLabelCol('review_score').\\\n",
    "    setFeaturesCol('tfidf').\\\n",
    "    setRegParam(0.0).\\\n",
    "    setMaxIter(100).\\\n",
    "    setElasticNetParam(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new pipeline to chain idf_pipeline with logistic regression\n",
    "# fit training set on pipeline\n",
    "lr_pipeline = Pipeline(stages=[tfidf_pipeline, lr]).fit(training_df)\n",
    "\n",
    "# precict on test and calculate accuracy\n",
    "lr_predictions = lr_pipeline.transform(testing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score above seems OK but now let's check the accuracy per class. we see it is not good for all but 5\n",
    "def printClassPredictions(predictions):\n",
    "    predictions.select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('Score = 1')\n",
    "    predictions.filter(predictions['review_score'] == 1).\\\n",
    "        select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('Score = 2')\n",
    "    predictions.filter(predictions['review_score'] == 2).\\\n",
    "        select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('Score = 3')\n",
    "    predictions.filter(predictions['review_score'] == 3).\\\n",
    "        select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('Score = 4')\n",
    "    predictions.filter(predictions['review_score'] == 4).\\\n",
    "        select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('Score = 5')\n",
    "    predictions.filter(predictions['review_score'] == 5).\\\n",
    "        select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printClassPredictions(lr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Logistic regression with elastic net regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-0c44d3c87263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0men_lr_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtfidf_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# fitting + accuracy estimation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0men_lr_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprintClassPredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_lr_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# not add elastic net regularization (combination of L1 and L2 reg)\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lambda_par = 0.1\n",
    "alpha_par = 0.3\n",
    "en_lr = LogisticRegression().\\\n",
    "        setLabelCol('review_score').\\\n",
    "        setFeaturesCol('tfidf').\\\n",
    "        setRegParam(lambda_par).\\\n",
    "        setMaxIter(100).\\\n",
    "        setElasticNetParam(alpha_par)\n",
    "\n",
    "# new pipeline to chain idf_pipeline with logistic regression\n",
    "en_lr_pipeline = Pipeline(stages=[tfidf_pipeline, en_lr]).fit(training_df)\n",
    "# fitting + accuracy estimation\n",
    "en_lr_predictions = lr_pipeline.transform(testing_df)\n",
    "\n",
    "printClassPredictions(en_lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some predictions for which the ground truth was score = 1\n",
    "en_lr_predictions.filter(en_lr_predictions['review_score'] == 1).\\\n",
    "    select(\"review_id\",\"review_concat\",\"review_score\",\"prediction\"). \\\n",
    "    show(n = 10, truncate = 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Logistic regression with stratified split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now make a new stratified split to make sure we have enough representative examples in the train set\n",
    "training_strat_df = s_df.sampleBy(\"review_score\", fractions={1: 0.8, 2: 0.8, 3: 0.8, 4: 0.8, 5: 0.8}, seed=42)\n",
    "test_strat_df = s_df.subtract(training_strat_df)\n",
    "# training set\n",
    "intersect = training_strat_df.select('review_id').intersect(test_strat_df.select('review_id'))\n",
    "print('size intersect: ' + str(intersect.count()))\n",
    "\n",
    "print('# rows training set: ' + str(training_strat_df.count()))\n",
    "print('# rows per class')\n",
    "training_strat_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()\n",
    "# test set\n",
    "print('# rows test set: ' + str(test_strat_df.count()))\n",
    "print('# rows per class')\n",
    "test_strat_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new prevision with previously defined en_lr (elestic net logistic regression)\n",
    "en_lr_strat_pipeline = Pipeline(stages=[tfidf_pipeline, en_lr]).fit(training_strat_df)\n",
    "# fitting + accuracy estimation\n",
    "predictions_en_lr_strat = en_lr_strat_pipeline.transform(test_strat_df)\n",
    "\n",
    "printClassPredictions(predictions_en_lr_strat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Logistic regression with down sampling\n",
    "Sampling down all class to the smallest 1 (= 1 star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recap data set size and distribution\n",
    "print('# rows per class')\n",
    "s_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()\n",
    "print('Total # rows in data set: '+ str(s_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling to 145 rows per class (# rows for class 1)\n",
    "downsampled_data = s_df.sampleBy('review_score',\n",
    "    fractions={1: 1, 2: 145./170, 3: 145./346, 4: 145./1529, 5: 145./9383}) \\\n",
    "    .cache()\n",
    "\n",
    "downsampled_data.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random split in train and test set with 80-20% proportions\n",
    "training_down_df, testing_down_df = downsampled_data.randomSplit([0.8, 0.2], seed=42)\n",
    "[training_down_df.count(), testing_down_df.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new prevision with previously defined en_lr (elestic net logistic regression)\n",
    "en_lr_down_pipeline = Pipeline(stages=[tfidf_pipeline, en_lr]).fit(training_down_df)\n",
    "# fitting + accuracy estimation\n",
    "predictions_en_lr_down = en_lr_down_pipeline.transform(testing_down_df)\n",
    "\n",
    "printClassPredictions(predictions_en_lr_down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Logistic regression with up and down sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using down - up sampling to build a train set with 2000 example per class\n",
    "# Test set = 20% of train set = 10000 * 0.2 = 1500 rows\n",
    "# Attention: test set cannot contains exmaple fro test set and must be built before up-down sampling\n",
    "#\n",
    "# Get test set of 2000 rows with a factor 'prop'\n",
    "prop = (11500.-2000.)/11500\n",
    "training_updown_df_pre = s_df.sampleBy(\"review_score\", fractions={1: prop, 2: prop, 3: prop, 4: prop, 5: prop}, seed=42)\n",
    "test_updown_df = s_df.subtract(training_updown_df_pre)\n",
    "# training set before up - down sampling\n",
    "print('training set before up - down sampling: ' + str(training_updown_df_pre.count()))\n",
    "training_updown_df_pre.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()\n",
    "# test set\n",
    "print('test set: ' + str(test_updown_df.count()))\n",
    "test_updown_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform up and down sampling on the trainig set so that each class contains +- 1500 rows\n",
    "df_class_1 = training_updown_df_pre[training_updown_df_pre['review_score'] == 1]\n",
    "df_class_2 = training_updown_df_pre[training_updown_df_pre['review_score'] == 2]\n",
    "df_class_3 = training_updown_df_pre[training_updown_df_pre['review_score'] == 3]\n",
    "df_class_4 = training_updown_df_pre[training_updown_df_pre['review_score'] == 4]\n",
    "df_class_5 = training_updown_df_pre[training_updown_df_pre['review_score'] == 5]\n",
    "\n",
    "df_class_1_over = df_class_1.sample(withReplacement=True, fraction=2000./117, seed = 42)\n",
    "df_class_2_over = df_class_2.sample(withReplacement=True, fraction=2000./139, seed = 42)\n",
    "df_class_3_over = df_class_3.sample(withReplacement=True, fraction=2000./290, seed = 42)\n",
    "df_class_4_over = df_class_4.sample(withReplacement=True, fraction=2000./1242, seed = 42)\n",
    "df_class_5_under = df_class_5.sample(withReplacement=True, fraction=2000./7721, seed = 42)\n",
    "\n",
    "import functools \n",
    "\n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1,df2: df1.union(df2.select(df1.columns)), dfs) \n",
    "\n",
    "training_updown_df = unionAll([df_class_1_over, df_class_2_over, df_class_3_over, df_class_4_over, df_class_5_under])\n",
    "\n",
    "print('# of rows in the train set')\n",
    "print('Total: ' + str(training_updown_df.count()))\n",
    "print('Per class:')\n",
    "training_updown_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new prevision with previously defined en_lr (elestic net logistic regression)\n",
    "en_lr_updown_pipeline = Pipeline(stages=[tfidf_pipeline, en_lr]).fit(training_updown_df)\n",
    "# fitting + accuracy estimation\n",
    "predictions_en_lr_updown = en_lr_updown_pipeline.transform(test_updown_df)\n",
    "\n",
    "printClassPredictions(predictions_en_lr_updown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.06213266162888329|\n",
      "+-------------------+\n",
      "\n",
      "Score = 1\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.06896551724137931|\n",
      "+-------------------+\n",
      "\n",
      "Score = 2\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.20512820512820512|\n",
      "+-------------------+\n",
      "\n",
      "Score = 3\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.25675675675675674|\n",
      "+-------------------+\n",
      "\n",
      "Score = 4\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.35522388059701493|\n",
      "+-------------------+\n",
      "\n",
      "Score = 5\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         0.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# on full data set\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1).\\\n",
    "        setLabelCol('review_score').\\\n",
    "        setFeaturesCol('tfidf')\n",
    "\n",
    "# new pipeline to chain idf_pipeline with logistic regression\n",
    "nb_pipeline = Pipeline(stages=[tfidf_pipeline, nb]).fit(training_df)\n",
    "# fitting + accuracy estimation\n",
    "nb_predictions = nb_pipeline.transform(testing_df)\n",
    "printClassPredictions(nb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        avg(correct)|\n",
      "+--------------------+\n",
      "|0.041561712846347604|\n",
      "+--------------------+\n",
      "\n",
      "Score = 1\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.22580645161290322|\n",
      "+-------------------+\n",
      "\n",
      "Score = 2\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.36363636363636365|\n",
      "+-------------------+\n",
      "\n",
      "Score = 3\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.13157894736842105|\n",
      "+-------------------+\n",
      "\n",
      "Score = 4\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.22364217252396165|\n",
      "+-------------------+\n",
      "\n",
      "Score = 5\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         0.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# on full data set\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1).\\\n",
    "        setLabelCol('review_score').\\\n",
    "        setFeaturesCol('tfidf')\n",
    "\n",
    "# new pipeline to chain idf_pipeline with logistic regression\n",
    "nb_pipeline_strat = Pipeline(stages=[tfidf_pipeline, nb]).fit(training_strat_df)\n",
    "# fitting + accuracy estimation\n",
    "nb_predictions_strat = nb_pipeline_strat.transform(test_strat_df)\n",
    "printClassPredictions(nb_predictions_strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size intersect: 0\n"
     ]
    }
   ],
   "source": [
    "intersect = training_strat_df.select('review_id').intersect(test_strat_df.select('review_id'))\n",
    "print('size intersect: ' + str(intersect.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on full data set\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "nb = NaiveBayes(smoothing=1).\\\n",
    "        setLabelCol('review_score').\\\n",
    "        setFeaturesCol('tfidf')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['tfidf','book_id'],outputCol=\"tfidf_book\")\n",
    "\n",
    "# new pipeline to chain idf_pipeline with logistic regression\n",
    "nb_pipeline = Pipeline(stages=[tfidf_pipeline, assembler, nb]).fit(training_strat_df)\n",
    "# fitting + accuracy estimation\n",
    "nb_predictions = lr_pipeline.transform(test_strat_df)\n",
    "printClassPredictions(nb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "predictions = rfModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"review_score\", outputCol=\"indexedLabel\").fit(s_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = s_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "assembler = VectorAssembler(inputCols=['tfidf','sentimentIndex'],outputCol=\"indexedFeatures\")\n",
    "\n",
    "# Train a GBT model.\n",
    "rf = RandomForestClassifier(labelCol=\"review_score\", featuresCol=\"indexedFeatures\", numTrees=1)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[tfidf_pipeline, assembler, rf])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"review_score\", \"indexedFeatures\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"review_score\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "gbtModel = model.stages[2]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Binary Classification\n",
    "## 4.1 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreToBin(value):\n",
    "   if   value < 4: return 0\n",
    "   else : return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "udfScoreToBin = udf(scoreToBin, IntegerType())\n",
    "s_df_bin = s_df.withColumn(\"bin_score\", udfScoreToBin(\"review_score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(book_id='0062678426', book_title='The Woman in the Window: A Novel', review_id='R1HMP34XP1V9BE', review_score=3, review_text='I wanted this to be better, it started so strong and then lost itself in the last third -to predictability.', review_title='Good, but not as good as the hype', review_user='Amazon Customer', timestamp=1557521653, review_concat='Good, but not as good as the hype I wanted this to be better, it started so strong and then lost itself in the last third -to predictability.', bin_score=0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# control that the function is properly working\n",
    "s_df_bin.where(fn.col('review_score') == 3).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows training set: 9191\n",
      "# rows per class\n",
      "+---------+-----+\n",
      "|bin_score|count|\n",
      "+---------+-----+\n",
      "|        1| 8670|\n",
      "|        0|  521|\n",
      "+---------+-----+\n",
      "\n",
      "# rows test set: 2382\n",
      "# rows per class\n",
      "+---------+-----+\n",
      "|bin_score|count|\n",
      "+---------+-----+\n",
      "|        1| 2242|\n",
      "|        0|  140|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now make a new stratified split to make sure we have enough representative examples in the train set\n",
    "training_strat_df_bin = s_df_bin.sampleBy(\"bin_score\", fractions={0: 0.8, 1: 0.8}, seed=42)\n",
    "test_strat_df_bin = s_df_bin.subtract(training_strat_df_bin)\n",
    "# training set\n",
    "\n",
    "print('# rows training set: ' + str(training_strat_df_bin.count()))\n",
    "print('# rows per class')\n",
    "training_strat_df_bin.groupBy(\"bin_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()\n",
    "# test set\n",
    "print('# rows test set: ' + str(test_strat_df_bin.count()))\n",
    "print('# rows per class')\n",
    "test_strat_df_bin.groupBy(\"bin_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score above seems OK but now let's check the accuracy per class. we see it is not good for all but 5\n",
    "def printClassPredictions(predictions):\n",
    "    predictions.select(fn.expr('float(prediction = bin_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('bin_score = 0')\n",
    "    predictions.filter(predictions['bin_score'] == 0).\\\n",
    "        select(fn.expr('float(prediction = bin_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('bin_score = 1')\n",
    "    predictions.filter(predictions['bin_score'] == 1).\\\n",
    "        select(fn.expr('float(prediction = bin_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.9412258606213266|\n",
      "+------------------+\n",
      "\n",
      "bin_score = 0\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         0.0|\n",
      "+------------+\n",
      "\n",
      "bin_score = 1\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         1.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lambda_par = 0.1\n",
    "alpha_par = 0.3\n",
    "en_lr_bin = LogisticRegression().\\\n",
    "        setLabelCol('bin_score').\\\n",
    "        setFeaturesCol('tfidf').\\\n",
    "        setRegParam(lambda_par).\\\n",
    "        setMaxIter(100).\\\n",
    "        setElasticNetParam(alpha_par)\n",
    "\n",
    "# new prevision with previously defined en_lr (elestic net logistic regression)\n",
    "en_lr_bin_pipeline = Pipeline(stages=[tfidf_pipeline, en_lr_bin]).fit(training_strat_df_bin)\n",
    "# fitting + accuracy estimation\n",
    "predictions_lr_bin = en_lr_bin_pipeline.transform(test_strat_df_bin)\n",
    "\n",
    "printClassPredictions(predictions_lr_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+------------+---------+----------+\n",
      "|                                                         review_concat|review_score|bin_score|prediction|\n",
      "+----------------------------------------------------------------------+------------+---------+----------+\n",
      "|Regirgitated motivational quotes, laughably phony If you read at a ...|           1|        0|       1.0|\n",
      "|Not my favorite Hungry Girl cook book Not crazy about this cook boo...|           2|        0|       1.0|\n",
      "|Disappointing and Petty I purchased this novel because I have alway...|           1|        0|       1.0|\n",
      "|Dark, dismal and overdone There is an overwhelming trend these days...|           2|        0|       1.0|\n",
      "|We need more gentlemen & women A nice read that made me smile and l...|           3|        0|       1.0|\n",
      "|A suspenseful pamphlet (Spoilers)My first issue is that it took 1/3...|           1|        0|       1.0|\n",
      "|Predictable I was underwhelmed by the book. The storyline was predi...|           3|        0|       1.0|\n",
      "|Conundrum There are so many good things about this book.  The writi...|           3|        0|       1.0|\n",
      "|Very slow. I am about halfway through this book, I am having hard t...|           2|        0|       1.0|\n",
      "|              Somewhat boring. Drug on far too long.  Somewhat boring.|           3|        0|       1.0|\n",
      "|Wish I hadn't wasted my time. There really wasn't anything I liked ...|           1|        0|       1.0|\n",
      "|Not a fan. Not feeling this book at all. Enjoyed the lightness off ...|           2|        0|       1.0|\n",
      "|Re-packaged information I'm not saying she doesn't have a good poin...|           2|        0|       1.0|\n",
      "|I enjoyed this book This book reminded me a lot of The Girl on the ...|           3|        0|       1.0|\n",
      "|Girl... you’re a mess She’s a mess, these books are a mess. Just sa...|           1|        0|       1.0|\n",
      "|Fantastic in parts Fantastic in parts, quite dull in others. I was ...|           3|        0|       1.0|\n",
      "|Missing the stickers The book itself has many attractive photos.  E...|           3|        0|       1.0|\n",
      "|Bad printing job This is a fun organizing book filled with rainbow ...|           2|        0|       1.0|\n",
      "|3 Stars The Woman in The Window by A.J. Finn was my most anticipate...|           3|        0|       1.0|\n",
      "|Waste of time This was poorly written and very predictable.  I don'...|           1|        0|       1.0|\n",
      "|Disappointed reader Interesting story line and amusing scenes...but...|           3|        0|       1.0|\n",
      "|Similar to “the girl on the train” Didn’t impress me at all, was ex...|           3|        0|       1.0|\n",
      "|Good, but not as good as the hype I wanted this to be better, it st...|           3|        0|       1.0|\n",
      "|Mixed. Feelings At first I did not like this book.  Then it drew me...|           3|        0|       1.0|\n",
      "|Tedious & Repetitious I was so excited about this book. There was s...|           2|        0|       1.0|\n",
      "|Not Gone Girl No suspense...  the ending wasn't a great surprise, m...|           3|        0|       1.0|\n",
      "|Packaged poorly I preordered this book & Waited months for this boo...|           1|        0|       1.0|\n",
      "|A winter read I felt that the characters were confusing even though...|           3|        0|       1.0|\n",
      "|Stick to the first 3 Let’s just say that if 2018 didn’t follow afte...|           1|        0|       1.0|\n",
      "|Good Book To Read If You're New to The Genre 3.75 stars - close to ...|           3|        0|       1.0|\n",
      "+----------------------------------------------------------------------+------------+---------+----------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show some predictions for which the ground truth was score = 1\n",
    "predictions_lr_bin.filter(predictions_lr_bin['bin_score'] == 0).\\\n",
    "    select(\"review_concat\",\"review_score\",\"bin_score\",\"prediction\"). \\\n",
    "    show(n = 30, truncate = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
