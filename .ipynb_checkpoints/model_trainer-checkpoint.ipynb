{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.45.217.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remarks / TO DO\n",
    "# 2. add to pre-processing stemming\n",
    "# 3. also try Naive Bayes and SVM (on top of log regr). See https://towardsdatascience.com/multi-class-text-classification-with-pyspark-7d78d022ed35\n",
    "# see also: http://classes.ischool.syr.edu/ist718/content/unit09/lab-sentiment_analysis/\n",
    "# 6. VERY IMPORTANT: I think we should rather reduce the number of categories from 5 to let's say 3\n",
    "# the 3 categories would be bad (0 and 1 star), middle (3 star), good (4 and 5 stars)\n",
    "# this would allow to have more training instance per categories and anyway how can even a human differentiate a 1 from a 2 stars or a 4 from a 5 stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data loading and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- book_id: integer (nullable = true)\n",
      " |-- book_title: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- review_score: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- review_title: string (nullable = true)\n",
      " |-- review_user: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- review_concat: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# start with easy implemetation: only consider the content of the 2 fields review_title and review_text\n",
    "# concantenate them in one new field \"review_concat\"from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "filepath = 'data_processed/ExctractedData.json'\n",
    "# load JSON file\n",
    "s_df = spark.read.json(filepath)\n",
    "# concatenate review text and title in one field\n",
    "s_df = s_df.withColumn('review_concat',fn.concat(fn.col('review_title'),fn.lit(' '), fn.col('review_text')))\n",
    "# review_score is of type String ==> cast it from String to Integer\n",
    "s_df = s_df.withColumn(\"review_score\", s_df[\"review_score\"].cast(IntegerType()))\n",
    "s_df = s_df.withColumn(\"book_id\", s_df[\"book_id\"].cast(IntegerType()))\n",
    "s_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of rows: 11573\n",
      "# of rows per class:\n",
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5| 9383|\n",
      "|           4| 1529|\n",
      "|           3|  346|\n",
      "|           2|  170|\n",
      "|           1|  145|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if duplicate review (normally not the case as the python script that filters the JSON took care of that)\n",
    "s_df = s_df.dropDuplicates(['review_id'])\n",
    "print('Total # of rows: ' + str(s_df.count()))\n",
    "print('# of rows per class:')\n",
    "s_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(book_id=62678426, book_title='The Woman in the Window: A Novel', review_id='R15DG6BI3K1I78', review_score=5, review_text=\"Extraordinary on any & every level. Astonishing that it' s a debut novel. Transfixing.\", review_title='Although reviews are universally stellar, highly recommend one avoids reading them & any synopsis preplunging in.', review_user='Perel Soreh', timestamp=1556661613, review_concat=\"Although reviews are universally stellar, highly recommend one avoids reading them & any synopsis preplunging in. Extraordinary on any & every level. Astonishing that it' s a debut novel. Transfixing.\")"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at first 5 star review\n",
    "s_df.where(fn.col('review_score') == 5).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(book_id=62824619, book_title='Cemetery Road: A Novel', review_id='R1T4O9RXIKX7D9', review_score=1, review_text='I am a huge fan of Greg Isles, but Cemetery Road was a outline of the garbage that the publishers must insist on before they will publish your book.  Mr. Isles, you are better than this, and you disappointed us with Cemetery Road.  I am going back to your older books, which are far superior to your latest endeavor.  In closing, there are no grey areas like you are suggesting in your book.  It is either moral or immoral.  There is no in between.', review_title='Disappointed', review_user='Jeanette Grayeb-Mihal', timestamp=1554878526, review_concat='Disappointed I am a huge fan of Greg Isles, but Cemetery Road was a outline of the garbage that the publishers must insist on before they will publish your book.  Mr. Isles, you are better than this, and you disappointed us with Cemetery Road.  I am going back to your older books, which are far superior to your latest endeavor.  In closing, there are no grey areas like you are suggesting in your book.  It is either moral or immoral.  There is no in between.')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at 1 very bad review\n",
    "s_df.where(fn.col('review_score') == 1).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(review_concat='Disappointed I am a huge fan of Greg Isles, but Cemetery Road was a outline of the garbage that the publishers must insist on before they will publish your book.  Mr. Isles, you are better than this, and you disappointed us with Cemetery Road.  I am going back to your older books, which are far superior to your latest endeavor.  In closing, there are no grey areas like you are suggesting in your book.  It is either moral or immoral.  There is no in between.')"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show ony review_concat field\n",
    "s_df.select('review_concat').where(fn.col('review_score') == 1).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import stop words to filter them out from the reviews\n",
    "import requests\n",
    "stop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()\n",
    "stop_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(tfidf=SparseVector(7909, {0: 0.9577, 7: 1.7476, 14: 2.1023, 42: 2.6845, 55: 2.8888, 138: 3.4816, 165: 3.6103, 201: 7.4795, 302: 4.3067, 462: 4.3459, 602: 4.8132, 1010: 10.2448, 1149: 5.1518, 1358: 5.5723, 1500: 5.5064, 2092: 12.7216, 2476: 6.0984, 2819: 6.1785, 3718: 6.5839, 3837: 6.7175, 4781: 6.7916, 6035: 7.1593, 6709: 7.2771, 7151: 7.4106, 7244: 7.4106, 7394: 15.1295}))"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define processing 4 steps and execute them with a trsnformation pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1. Tokenizer, .setPattern(\"\\\\p{L}+\") means that it remove accent from words (check it has no impact on the smileys !!!)\n",
    "tokenizer = RegexTokenizer().setGaps(False)\\\n",
    "  .setPattern(\"\\\\p{L}+\")\\\n",
    "  .setInputCol(\"review_concat\")\\\n",
    "  .setOutputCol(\"words\")\n",
    "\n",
    "# 2. filter out stop words\n",
    "sw_filter = StopWordsRemover()\\\n",
    "  .setStopWords(stop_words)\\\n",
    "  .setCaseSensitive(False)\\\n",
    "  .setInputCol(\"words\")\\\n",
    "  .setOutputCol(\"filtered\")\n",
    "\n",
    "# 3. TF: TF vectorization + remove words that appear in 5 docs or less\n",
    "#  converts text documents to vectors of term counts\n",
    "cv = CountVectorizer(minTF=1., minDF=5., vocabSize=10000)\\\n",
    "  .setInputCol(\"filtered\")\\\n",
    "  .setOutputCol(\"tf\")\n",
    "\n",
    "# 4. TF-IDF transform\n",
    "# The IDFModel takes feature vectors (generally created from HashingTF or CountVectorizer) and scales each column. \n",
    "# Intuitively, it down-weights columns which appear frequently in a corpus.\n",
    "idf = IDF().\\\n",
    "    setInputCol('tf').\\\n",
    "    setOutputCol('tfidf')\n",
    "\n",
    "# Create a pipelined transformer and fit it with full data set\n",
    "tfidf_pipeline = Pipeline(stages=[tokenizer, sw_filter, cv, idf]).fit(s_df)\n",
    "\n",
    "# Control execution of preprocessing pipeline by pre-processing the data\n",
    "s_df_transform = tfidf_pipeline.transform(s_df)\n",
    "s_df_transform.select('tfidf').where(fn.col('review_score') == 1).first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- book_id: integer (nullable = true)\n",
      " |-- book_title: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- review_score: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- review_title: string (nullable = true)\n",
      " |-- review_user: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- review_concat: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tf: vector (nullable = true)\n",
      " |-- tfidf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check schema of output of preprocessing pipeline \n",
    "s_df_transform.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model training\n",
    "## 3.1. Simple logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9191, 2382]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random split in train and test set with 80-20% proportions\n",
    "training_df, testing_df = s_df.randomSplit([0.8, 0.2], seed=42)\n",
    "[training_df.count(), testing_df.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add logistic regression to the previously defined pipeline\n",
    "lr = LogisticRegression().\\\n",
    "    setLabelCol('review_score').\\\n",
    "    setFeaturesCol('tfidf').\\\n",
    "    setRegParam(0.0).\\\n",
    "    setMaxIter(100).\\\n",
    "    setElasticNetParam(0.)\n",
    "\n",
    "# new pipeline to chain idf_pipeline with logistic regression\n",
    "# fit training set on pipeline\n",
    "lr_pipeline = Pipeline(stages=[tfidf_pipeline, lr]).fit(training_df)\n",
    "\n",
    "# precict on test and calculate accuracy\n",
    "lr_predictions = lr_pipeline.transform(testing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.8211586901763224|\n",
      "+------------------+\n",
      "\n",
      "Score = 1\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.27586206896551724|\n",
      "+-------------------+\n",
      "\n",
      "Score = 2\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.20512820512820512|\n",
      "+-------------------+\n",
      "\n",
      "Score = 3\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.33783783783783783|\n",
      "+-------------------+\n",
      "\n",
      "Score = 4\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.5701492537313433|\n",
      "+------------------+\n",
      "\n",
      "Score = 5\n",
      "+----------------+\n",
      "|    avg(correct)|\n",
      "+----------------+\n",
      "|0.90498687664042|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# score above seems OK but now let's check the accuracy per class. we see it is not good for all but 5\n",
    "def printClassPredictions(predictions):\n",
    "    predictions.select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('Score = 1')\n",
    "    predictions.filter(predictions['review_score'] == 1).\\\n",
    "        select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('Score = 2')\n",
    "    predictions.filter(predictions['review_score'] == 2).\\\n",
    "        select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('Score = 3')\n",
    "    predictions.filter(predictions['review_score'] == 3).\\\n",
    "        select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('Score = 4')\n",
    "    predictions.filter(predictions['review_score'] == 4).\\\n",
    "        select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('Score = 5')\n",
    "    predictions.filter(predictions['review_score'] == 5).\\\n",
    "        select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    \n",
    "printClassPredictions(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Logistic regression with elastic net regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.8207388748950462|\n",
      "+------------------+\n",
      "\n",
      "Score = 1\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.27586206896551724|\n",
      "+-------------------+\n",
      "\n",
      "Score = 2\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.20512820512820512|\n",
      "+-------------------+\n",
      "\n",
      "Score = 3\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.33783783783783783|\n",
      "+-------------------+\n",
      "\n",
      "Score = 4\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.5671641791044776|\n",
      "+------------------+\n",
      "\n",
      "Score = 5\n",
      "+----------------+\n",
      "|    avg(correct)|\n",
      "+----------------+\n",
      "|0.90498687664042|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# not add elastic net regularization (combination of L1 and L2 reg)\n",
    "lambda_par = 0.1\n",
    "alpha_par = 0.3\n",
    "en_lr = LogisticRegression().\\\n",
    "        setLabelCol('review_score').\\\n",
    "        setFeaturesCol('tfidf').\\\n",
    "        setRegParam(lambda_par).\\\n",
    "        setMaxIter(100).\\\n",
    "        setElasticNetParam(alpha_par)\n",
    "\n",
    "# new pipeline to chain idf_pipeline with logistic regression\n",
    "en_lr_pipeline = Pipeline(stages=[tfidf_pipeline, en_lr]).fit(training_df)\n",
    "# fitting + accuracy estimation\n",
    "en_lr_predictions = lr_pipeline.transform(testing_df)\n",
    "\n",
    "printClassPredictions(en_lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------------------------------------------------------+------------+----------+\n",
      "|     review_id|                                                         review_concat|review_score|prediction|\n",
      "+--------------+----------------------------------------------------------------------+------------+----------+\n",
      "|R2TPIP9WFJFHBF|Blurry. Do not purchase. I just received this today. Do not be dece...|           1|       5.0|\n",
      "|R24G49195RDQSV|Waste of time Extremely disappointed in this book.  Do not understa...|           1|       1.0|\n",
      "|R18E85EKCWU53F|A complete waste of my time Soooo tedious. The drinking, the pills,...|           1|       2.0|\n",
      "|R24O439CRE9HHV|A Weeping Liberal Who has a typical weeping liberals non grasp of i...|           1|       5.0|\n",
      "|  R50JW2WMC3O4|Good idea but executed poorly Started off pretty well but then kept...|           1|       4.0|\n",
      "|R1XHIL1UCA5F8Y|The agony of reading this book This is beautifully written but so, ...|           1|       3.0|\n",
      "|R37KN1H2QR3MU7|Don’t bother You will need a magnifying glass to read this...very d...|           1|       5.0|\n",
      "|R3RIUDF9EGPKOY|Manipulative tripe If you want to spend time with an alcoholic pill...|           1|       2.0|\n",
      "|R1IHWAJIEV7VMM|Don’t bother.  Overhyped! I never write book reviews but I am moved...|           1|       1.0|\n",
      "|R16DWJL44S5LGL|Hate Tessa The biggest problem for me about this books is that the ...|           1|       1.0|\n",
      "+--------------+----------------------------------------------------------------------+------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show some predictions for which the ground truth was score = 1\n",
    "predictions.filter(predictions['review_score'] == 1).\\\n",
    "    select(\"review_id\",\"review_concat\",\"review_score\",\"prediction\"). \\\n",
    "    show(n = 10, truncate = 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Logistic regression with stratified split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows training set: 9191\n",
      "# rows per class\n",
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5| 7459|\n",
      "|           4| 1206|\n",
      "|           3|  280|\n",
      "|           2|  134|\n",
      "|           1|  112|\n",
      "+------------+-----+\n",
      "\n",
      "# rows test set: 2382\n",
      "# rows per class\n",
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5| 1929|\n",
      "|           4|  313|\n",
      "|           3|   76|\n",
      "|           2|   33|\n",
      "|           1|   31|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now make a new stratified split to make sure we have enough representative examples in the train set\n",
    "training_strat_df = s_df.sampleBy(\"review_score\", fractions={1: 0.8, 2: 0.8, 3: 0.8, 4: 0.8, 5: 0.8}, seed=42)\n",
    "test_strat_df = s_df.subtract(training_strat_df)\n",
    "# training set\n",
    "print('# rows training set: ' + str(training_strat_df.count()))\n",
    "print('# rows per class')\n",
    "training_strat_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()\n",
    "# test set\n",
    "print('# rows test set: ' + str(test_strat_df.count()))\n",
    "print('# rows per class')\n",
    "test_strat_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.8434089000839631|\n",
      "+------------------+\n",
      "\n",
      "Score = 1\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.03225806451612903|\n",
      "+-------------------+\n",
      "\n",
      "Score = 2\n",
      "+--------------------+\n",
      "|        avg(correct)|\n",
      "+--------------------+\n",
      "|0.030303030303030304|\n",
      "+--------------------+\n",
      "\n",
      "Score = 3\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.05263157894736842|\n",
      "+-------------------+\n",
      "\n",
      "Score = 4\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.24920127795527156|\n",
      "+-------------------+\n",
      "\n",
      "Score = 5\n",
      "+-----------------+\n",
      "|     avg(correct)|\n",
      "+-----------------+\n",
      "|0.997926386728875|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new prevision with previously defined en_lr (elestic net logistic regression)\n",
    "en_lr_strat_pipeline = Pipeline(stages=[tfidf_pipeline, en_lr]).fit(training_strat_df)\n",
    "# fitting + accuracy estimation\n",
    "predictions_en_lr_strat = en_lr_strat_pipeline.transform(test_strat_df)\n",
    "\n",
    "printClassPredictions(predictions_en_lr_strat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Logistic regression with down sampling\n",
    "Sampling down all class to the smallest 1 (= 1 star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows per class\n",
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5| 9383|\n",
      "|           4| 1529|\n",
      "|           3|  346|\n",
      "|           2|  170|\n",
      "|           1|  145|\n",
      "+------------+-----+\n",
      "\n",
      "Total # rows in data set: 11573\n"
     ]
    }
   ],
   "source": [
    "# recap data set size and distribution\n",
    "print('# rows per class')\n",
    "s_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()\n",
    "print('Total # rows in data set: '+ str(s_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5|  168|\n",
      "|           3|  148|\n",
      "|           4|  147|\n",
      "|           2|  147|\n",
      "|           1|  145|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# downsampling to 145 rows per class (# rows for class 1)\n",
    "downsampled_data = s_df.sampleBy('review_score',\n",
    "    fractions={1: 1, 2: 145./170, 3: 145./346, 4: 145./1529, 5: 145./9383}) \\\n",
    "    .cache()\n",
    "\n",
    "downsampled_data.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[595, 160]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random split in train and test set with 80-20% proportions\n",
    "training_down_df, testing_down_df = downsampled_data.randomSplit([0.8, 0.2], seed=42)\n",
    "[training_down_df.count(), testing_down_df.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|      0.4375|\n",
      "+------------+\n",
      "\n",
      "Score = 1\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         0.5|\n",
      "+------------+\n",
      "\n",
      "Score = 2\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.37037037037037035|\n",
      "+-------------------+\n",
      "\n",
      "Score = 3\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         0.5|\n",
      "+------------+\n",
      "\n",
      "Score = 4\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.3333333333333333|\n",
      "+------------------+\n",
      "\n",
      "Score = 5\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.4782608695652174|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new prevision with previously defined en_lr (elestic net logistic regression)\n",
    "en_lr_down_pipeline = Pipeline(stages=[tfidf_pipeline, en_lr]).fit(training_down_df)\n",
    "# fitting + accuracy estimation\n",
    "predictions_en_lr_down = en_lr_down_pipeline.transform(testing_down_df)\n",
    "\n",
    "printClassPredictions(predictions_en_lr_down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Logistic regression with up and down sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set before up - down sampling: 9509\n",
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5| 7721|\n",
      "|           4| 1242|\n",
      "|           3|  290|\n",
      "|           2|  139|\n",
      "|           1|  117|\n",
      "+------------+-----+\n",
      "\n",
      "test set: 2064\n",
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5| 1659|\n",
      "|           4|  275|\n",
      "|           3|   72|\n",
      "|           2|   32|\n",
      "|           1|   26|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using down - up sampling to build a train set with 2000 example per class\n",
    "# Test set = 20% of train set = 10000 * 0.2 = 1500 rows\n",
    "# Attention: test set cannot contains exmaple fro test set and must be built before up-down sampling\n",
    "#\n",
    "# Get test set of 2000 rows with a factor 'prop'\n",
    "prop = (11500.-2000.)/11500\n",
    "training_updown_df_pre = s_df.sampleBy(\"review_score\", fractions={1: prop, 2: prop, 3: prop, 4: prop, 5: prop}, seed=42)\n",
    "test_updown_df = s_df.subtract(training_updown_df_pre)\n",
    "# training set before up - down sampling\n",
    "print('training set before up - down sampling: ' + str(training_updown_df_pre.count()))\n",
    "training_updown_df_pre.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()\n",
    "# test set\n",
    "print('test set: ' + str(test_updown_df.count()))\n",
    "test_updown_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows in the train set\n",
      "Total: 9798\n",
      "Per class:\n",
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           4| 2002|\n",
      "|           3| 1976|\n",
      "|           1| 1960|\n",
      "|           2| 1937|\n",
      "|           5| 1923|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# perform up and down sampling on the trainig set so that each class contains +- 1500 rows\n",
    "df_class_1 = training_updown_df_pre[training_updown_df_pre['review_score'] == 1]\n",
    "df_class_2 = training_updown_df_pre[training_updown_df_pre['review_score'] == 2]\n",
    "df_class_3 = training_updown_df_pre[training_updown_df_pre['review_score'] == 3]\n",
    "df_class_4 = training_updown_df_pre[training_updown_df_pre['review_score'] == 4]\n",
    "df_class_5 = training_updown_df_pre[training_updown_df_pre['review_score'] == 5]\n",
    "\n",
    "df_class_1_over = df_class_1.sample(withReplacement=True, fraction=2000./117, seed = 42)\n",
    "df_class_2_over = df_class_2.sample(withReplacement=True, fraction=2000./139, seed = 42)\n",
    "df_class_3_over = df_class_3.sample(withReplacement=True, fraction=2000./290, seed = 42)\n",
    "df_class_4_over = df_class_4.sample(withReplacement=True, fraction=2000./1242, seed = 42)\n",
    "df_class_5_under = df_class_5.sample(withReplacement=True, fraction=2000./7721, seed = 42)\n",
    "\n",
    "import functools \n",
    "\n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1,df2: df1.union(df2.select(df1.columns)), dfs) \n",
    "\n",
    "training_updown_df = unionAll([df_class_1_over, df_class_2_over, df_class_3_over, df_class_4_over, df_class_5_under])\n",
    "\n",
    "print('# of rows in the train set')\n",
    "print('Total: ' + str(training_updown_df.count()))\n",
    "print('Per class:')\n",
    "training_updown_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.4874031007751938|\n",
      "+------------------+\n",
      "\n",
      "Score = 1\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.6538461538461539|\n",
      "+------------------+\n",
      "\n",
      "Score = 2\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|        0.25|\n",
      "+------------+\n",
      "\n",
      "Score = 3\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.3888888888888889|\n",
      "+------------------+\n",
      "\n",
      "Score = 4\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.5454545454545454|\n",
      "+------------------+\n",
      "\n",
      "Score = 5\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.48402652200120555|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new prevision with previously defined en_lr (elestic net logistic regression)\n",
    "en_lr_updown_pipeline = Pipeline(stages=[tfidf_pipeline, en_lr]).fit(training_updown_df)\n",
    "# fitting + accuracy estimation\n",
    "predictions_en_lr_updown = en_lr_updown_pipeline.transform(test_updown_df)\n",
    "\n",
    "printClassPredictions(predictions_en_lr_updown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.9563392107472712|\n",
      "+------------------+\n",
      "\n",
      "Score = 1\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.8709677419354839|\n",
      "+------------------+\n",
      "\n",
      "Score = 2\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.8484848484848485|\n",
      "+------------------+\n",
      "\n",
      "Score = 3\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.8552631578947368|\n",
      "+------------------+\n",
      "\n",
      "Score = 4\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.8690095846645367|\n",
      "+------------------+\n",
      "\n",
      "Score = 5\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.9777086573354069|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# on full data set\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1).\\\n",
    "        setLabelCol('review_score').\\\n",
    "        setFeaturesCol('tfidf')\n",
    "\n",
    "#book_id\n",
    "\n",
    "# new pipeline to chain idf_pipeline with logistic regression\n",
    "nb_pipeline = Pipeline(stages=[tfidf_pipeline, nb]).fit(training_strat_df)\n",
    "# fitting + accuracy estimation\n",
    "nb_predictions = lr_pipeline.transform(test_strat_df)\n",
    "printClassPredictions(nb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Data type string of column book_id is not supported.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o13470.transform.\n: java.lang.IllegalArgumentException: Data type string of column book_id is not supported.\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:169)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:86)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-50c35bed2aa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# new pipeline to chain idf_pipeline with logistic regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnb_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtfidf_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_strat_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# fitting + accuracy estimation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mnb_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_strat_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/Documents/spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Data type string of column book_id is not supported.'"
     ]
    }
   ],
   "source": [
    "# on full data set\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "nb = NaiveBayes(smoothing=1).\\\n",
    "        setLabelCol('review_score').\\\n",
    "        setFeaturesCol('tfidf')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['tfidf','book_id'],outputCol=\"tfidf_book\")\n",
    "\n",
    "# new pipeline to chain idf_pipeline with logistic regression\n",
    "nb_pipeline = Pipeline(stages=[tfidf_pipeline, assembler, nb]).fit(training_strat_df)\n",
    "# fitting + accuracy estimation\n",
    "nb_predictions = lr_pipeline.transform(test_strat_df)\n",
    "printClassPredictions(nb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "predictions = rfModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VectorAssembler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-50d21d17816f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Automatically identify categorical features, and index them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0massembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentimentIndex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"indexedFeatures\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Train a GBT model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VectorAssembler' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"review_score\", outputCol=\"indexedLabel\").fit(s_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = s_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "assembler = VectorAssembler(inputCols=['tfidf','sentimentIndex'],outputCol=\"indexedFeatures\")\n",
    "\n",
    "# Train a GBT model.\n",
    "rf = RandomForestClassifier(labelCol=\"review_score\", featuresCol=\"indexedFeatures\", numTrees=1)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[tfidf_pipeline, assembler, rf])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"review_score\", \"indexedFeatures\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"review_score\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "gbtModel = model.stages[2]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model with multiples inputs\n",
    "## 4.1 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
