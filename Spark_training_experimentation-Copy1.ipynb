{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://admins-air.lan:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data loading and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with easy implemetation: only consider the content of the 2 fields review_title and review_text\n",
    "# concantenate them in one new field \"review_concat\"from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "filepath = 'data_processed/ExctractedData.json'\n",
    "# load JSON file\n",
    "s_df = spark.read.json(filepath)\n",
    "s_df.count()\n",
    "s_df = s_df.drop_duplicates(subset=['review_id'])\n",
    "pd_df = s_df.groupBy('review_id').count().toPandas().set_index(\"count\").sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R15DG6BI3K1I78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R1UU50BM0S4LPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R27KEMBTEQ4MHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R1HMP34XP1V9BE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R22I2JYOOXA3PP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            review_id\n",
       "count                \n",
       "1      R15DG6BI3K1I78\n",
       "1      R1UU50BM0S4LPY\n",
       "1      R27KEMBTEQ4MHI\n",
       "1      R1HMP34XP1V9BE\n",
       "1      R22I2JYOOXA3PP"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# control no duplicate\n",
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- book_title: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- review_score: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- review_title: string (nullable = true)\n",
      " |-- review_user: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- review_concat: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# concatenate review text and title in one field\n",
    "s_df = s_df.withColumn('review_concat',fn.concat(fn.col('review_title'),fn.lit(' '), fn.col('review_text')))\n",
    "# review_score is of type String ==> cast it from String to Integer\n",
    "s_df = s_df.withColumn(\"review_score\", s_df[\"review_score\"].cast(IntegerType()))\n",
    "#s_df = s_df.withColumn(\"book_id\", s_df[\"book_id\"].cast(IntegerType()))\n",
    "s_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of rows: 11573\n",
      "# of rows per class:\n",
      "+--------------------+-----+\n",
      "|         review_user|count|\n",
      "+--------------------+-----+\n",
      "|        D.P. McHenry|  505|\n",
      "|     Amazon Customer|  410|\n",
      "|The Guide To Roma...|  251|\n",
      "|Beckyrae99 (Becky...|  189|\n",
      "|              Liz R.|  169|\n",
      "|         Keith Hauge|  156|\n",
      "|     Kindle Customer|  150|\n",
      "|              Sheela|  117|\n",
      "|            MaureenB|  115|\n",
      "|         JB VonShirl|  108|\n",
      "|            Daniotra|  106|\n",
      "|           Mary Lins|  102|\n",
      "|         Nolia Nessa|  101|\n",
      "|     Paul A. Johnson|   97|\n",
      "|           R. Zocher|   95|\n",
      "|Jessica Sotelo (A...|   89|\n",
      "|         KindleReads|   82|\n",
      "|     Keith A. Comess|   75|\n",
      "|             Eric M.|   73|\n",
      "|        Lisa Kersten|   72|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Total # of rows: ' + str(s_df.count()))\n",
    "print('# of rows per class:')\n",
    "s_df.groupBy(\"review_user\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+-----+\n",
      "|review_user                                    |count|\n",
      "+-----------------------------------------------+-----+\n",
      "|D.P. McHenry                                   |505  |\n",
      "|Amazon Customer                                |410  |\n",
      "|The Guide To Romance Novels                    |251  |\n",
      "|Beckyrae99 (Becky Wise)                        |189  |\n",
      "|Liz R.                                         |169  |\n",
      "|Keith Hauge                                    |156  |\n",
      "|Kindle Customer                                |150  |\n",
      "|Sheela                                         |117  |\n",
      "|MaureenB                                       |115  |\n",
      "|JB VonShirl                                    |108  |\n",
      "|Daniotra                                       |106  |\n",
      "|Mary Lins                                      |102  |\n",
      "|Nolia Nessa                                    |101  |\n",
      "|Paul A. Johnson                                |97   |\n",
      "|R. Zocher                                      |95   |\n",
      "|Jessica Sotelo (Angie & Jessica's Dreamy Reads)|89   |\n",
      "|KindleReads                                    |82   |\n",
      "|Keith A. Comess                                |75   |\n",
      "|Eric M.                                        |73   |\n",
      "|Lisa Kersten                                   |72   |\n",
      "+-----------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check how many review per user\n",
    "s_df.groupBy(\"review_user\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min / max / avg review score for top reviewer (100 or more reviews)\n",
      "+--------------------+--------------------+-----------------+-----------------+------------------+\n",
      "|         review_user|         review_user|min(review_score)|max(review_score)| avg(review_score)|\n",
      "+--------------------+--------------------+-----------------+-----------------+------------------+\n",
      "|         Nolia Nessa|         Nolia Nessa|                5|                5|               5.0|\n",
      "|Beckyrae99 (Becky...|Beckyrae99 (Becky...|                4|                4|               4.0|\n",
      "|           Mary Lins|           Mary Lins|                5|                5|               5.0|\n",
      "|              Liz R.|              Liz R.|                5|                5|               5.0|\n",
      "|         Keith Hauge|         Keith Hauge|                5|                5|               5.0|\n",
      "|            MaureenB|            MaureenB|                5|                5|               5.0|\n",
      "|              Sheela|              Sheela|                5|                5|               5.0|\n",
      "|        D.P. McHenry|        D.P. McHenry|                5|                5|               5.0|\n",
      "|     Kindle Customer|     Kindle Customer|                1|                5|              4.52|\n",
      "|The Guide To Roma...|The Guide To Roma...|                5|                5|               5.0|\n",
      "|     Amazon Customer|     Amazon Customer|                1|                5|4.4365853658536585|\n",
      "|         JB VonShirl|         JB VonShirl|                5|                5|               5.0|\n",
      "|            Daniotra|            Daniotra|                5|                5|               5.0|\n",
      "+--------------------+--------------------+-----------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check min / max / avg rating for top reviewer ==> we see they all give 5 star reviews\n",
    "print('Min / max / avg review score for top reviewer (100 or more reviews)')\n",
    "s_df.select(\"review_user\", \"review_score\") \\\n",
    "    .groupBy(\"review_user\").agg( s_df.review_user, fn.min(\"review_score\"), fn.max(\"review_score\"), fn.avg(\"review_score\") ) \\\n",
    "    .where( (fn.col('review_user') == 'D.P. McHenry') | (fn.col('review_user') == 'Amazon Customer') \\\n",
    "           | (fn.col('review_user') == 'The Guide To Romance Novels') | (fn.col('review_user') == 'Beckyrae99 (Becky Wise)')  \\\n",
    "           | (fn.col('review_user') == 'Liz R.') | (fn.col('review_user') == 'Keith Hauge')  \\\n",
    "           | (fn.col('review_user') == 'Kindle Customer') | (fn.col('review_user') == 'Sheela')  \\\n",
    "           | (fn.col('review_user') == 'MaureenB') | (fn.col('review_user') == 'JB VonShirl')  \\\n",
    "           | (fn.col('review_user') == 'Daniotra') | (fn.col('review_user') == 'Mary Lins') | (fn.col('review_user') == 'Nolia Nessa'))  \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|book_id   |count|\n",
      "+----------+-----+\n",
      "|0143110438|2129 |\n",
      "|0062678426|1071 |\n",
      "|198211598X|579  |\n",
      "|62678426  |506  |\n",
      "|1984898329|251  |\n",
      "|0800736524|242  |\n",
      "|0525538194|242  |\n",
      "|0525572643|231  |\n",
      "|1542046513|189  |\n",
      "|62319795  |176  |\n",
      "|0553448234|169  |\n",
      "|1400209609|158  |\n",
      "|0525536582|156  |\n",
      "|1400208017|134  |\n",
      "|162860378X|117  |\n",
      "|1644450003|117  |\n",
      "|0310353629|115  |\n",
      "|194883605X|108  |\n",
      "|0393239861|106  |\n",
      "|0525436146|102  |\n",
      "|0316414212|101  |\n",
      "|1982111003|97   |\n",
      "|1607749580|97   |\n",
      "|0316316121|95   |\n",
      "|B071SBMK94|89   |\n",
      "+----------+-----+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check how many review per book\n",
    "s_df.groupBy(\"book_id\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show(25, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min / max / avg review score for top books\n",
      "+----------+----------+-----------------+-----------------+------------------+\n",
      "|   book_id|   book_id|min(review_score)|max(review_score)| avg(review_score)|\n",
      "+----------+----------+-----------------+-----------------+------------------+\n",
      "|0553448234|0553448234|                5|                5|               5.0|\n",
      "|  62678426|  62678426|                1|                5|4.2272727272727275|\n",
      "|0525538194|0525538194|                1|                5| 4.743801652892562|\n",
      "|198211598X|198211598X|                1|                5|4.6217616580310885|\n",
      "|0525536582|0525536582|                5|                5|               5.0|\n",
      "|1400209609|1400209609|                1|                5| 4.234177215189874|\n",
      "|0143110438|0143110438|                1|                5| 4.789572569281352|\n",
      "|1542046513|1542046513|                4|                4|               4.0|\n",
      "|  62319795|  62319795|                1|                5| 4.482954545454546|\n",
      "|0062678426|0062678426|                1|                5| 4.330532212885154|\n",
      "|0800736524|0800736524|                4|                5|4.9958677685950414|\n",
      "|0525572643|0525572643|                1|                5| 4.909090909090909|\n",
      "|1984898329|1984898329|                5|                5|               5.0|\n",
      "+----------+----------+-----------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check min rating for top reviewer ==> we see they all give 5 star reviews\n",
    "#    .groupBy(\"book_id\").agg( s_df.book_id, f.min(\"review_score\"), f.max(\"review_score\"), f.avg(\"review_score\") ) \\\n",
    "print('Min / max / avg review score for top books')\n",
    "s_df.select(\"book_id\", \"review_score\") \\\n",
    "    .groupBy(\"book_id\").agg( s_df.book_id, fn.min(\"review_score\"), fn.max(\"review_score\"), fn.avg(\"review_score\") ) \\\n",
    "    .where( (fn.col('book_id') == '0143110438') | (fn.col('book_id') == '0062678426') \\\n",
    "           | (fn.col('book_id') == '198211598X') | (fn.col('book_id') == '62678426')  \\\n",
    "           | (fn.col('book_id') == '1984898329') | (fn.col('book_id') == '0525538194')  \\\n",
    "           | (fn.col('book_id') == '0800736524') | (fn.col('book_id') == '0525572643')  \\\n",
    "           | (fn.col('book_id') == '1542046513') | (fn.col('book_id') == '62319795')  \\\n",
    "           | (fn.col('book_id') == '0553448234') | (fn.col('book_id') == '1400209609') | (fn.col('book_id') == '0525536582'))  \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(book_id='0062678426', book_title='The Woman in the Window: A Novel', review_id='R15DG6BI3K1I78', review_score=5, review_text=\"Extraordinary on any & every level. Astonishing that it' s a debut novel. Transfixing.\", review_title='Although reviews are universally stellar, highly recommend one avoids reading them & any synopsis preplunging in.', review_user='Perel Soreh', timestamp=1556661613, review_concat=\"Although reviews are universally stellar, highly recommend one avoids reading them & any synopsis preplunging in. Extraordinary on any & every level. Astonishing that it' s a debut novel. Transfixing.\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at first 5 star review\n",
    "s_df.where(fn.col('review_score') == 5).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(book_id='0062824619', book_title='Cemetery Road: A Novel', review_id='R1T4O9RXIKX7D9', review_score=1, review_text='I am a huge fan of Greg Isles, but Cemetery Road was a outline of the garbage that the publishers must insist on before they will publish your book.  Mr. Isles, you are better than this, and you disappointed us with Cemetery Road.  I am going back to your older books, which are far superior to your latest endeavor.  In closing, there are no grey areas like you are suggesting in your book.  It is either moral or immoral.  There is no in between.', review_title='Disappointed', review_user='Jeanette Grayeb-Mihal', timestamp=1554878526, review_concat='Disappointed I am a huge fan of Greg Isles, but Cemetery Road was a outline of the garbage that the publishers must insist on before they will publish your book.  Mr. Isles, you are better than this, and you disappointed us with Cemetery Road.  I am going back to your older books, which are far superior to your latest endeavor.  In closing, there are no grey areas like you are suggesting in your book.  It is either moral or immoral.  There is no in between.')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at 1 very bad review\n",
    "s_df.where(fn.col('review_score') == 1).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(review_concat='Disappointed I am a huge fan of Greg Isles, but Cemetery Road was a outline of the garbage that the publishers must insist on before they will publish your book.  Mr. Isles, you are better than this, and you disappointed us with Cemetery Road.  I am going back to your older books, which are far superior to your latest endeavor.  In closing, there are no grey areas like you are suggesting in your book.  It is either moral or immoral.  There is no in between.')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show ony review_concat field\n",
    "s_df.select('review_concat').where(fn.col('review_score') == 1).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import stop words to filter them out from the reviews\n",
    "import requests\n",
    "stop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()\n",
    "stop_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a83ef09a64d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 2. filter out stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msw_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStopWordsRemover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0;34m.\u001b[0m\u001b[0msetStopWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0msetCaseSensitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0msetInputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"words\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "# define processing 4 steps and execute them with a trsnformation pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1. Tokenizer, .setPattern(\"\\\\p{L}+\") means that it remove accent from words (check it has no impact on the smileys !!!)\n",
    "tokenizer = RegexTokenizer().setGaps(False)\\\n",
    "  .setPattern(\"\\\\p{L}+\")\\\n",
    "  .setInputCol(\"review_concat\")\\\n",
    "  .setOutputCol(\"words\")\n",
    "\n",
    "# 2. filter out stop words\n",
    "sw_filter = StopWordsRemover()\\\n",
    "  .setStopWords(stop_words)\\\n",
    "  .setCaseSensitive(False)\\\n",
    "  .setInputCol(\"words\")\\\n",
    "  .setOutputCol(\"filtered\")\n",
    "\n",
    "# 3. TF: TF vectorization + remove words that appear in 5 docs or less\n",
    "#  converts text documents to vectors of term counts\n",
    "cv = CountVectorizer(minTF=1., minDF=5., vocabSize=2**17)\\\n",
    "  .setInputCol(\"filtered\")\\\n",
    "  .setOutputCol(\"tf\")\n",
    "\n",
    "# 4. TF-IDF transform\n",
    "# The IDFModel takes feature vectors (generally created from HashingTF or CountVectorizer) and scales each column. \n",
    "# Intuitively, it down-weights columns which appear frequently in a corpus.\n",
    "idf = IDF().\\\n",
    "    setInputCol('tf').\\\n",
    "    setOutputCol('tfidf')\n",
    "\n",
    "# Create a pipelined transformer and fit it with full data set\n",
    "tfidf_pipeline = Pipeline(stages=[tokenizer, sw_filter, cv, idf]).fit(s_df)\n",
    "\n",
    "# Control execution of preprocessing pipeline by pre-processing the data\n",
    "s_df_transform = tfidf_pipeline.transform(s_df)\n",
    "s_df_transform.select('tfidf').where(fn.col('review_score') == 1).first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- book_title: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- review_score: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- review_title: string (nullable = true)\n",
      " |-- review_user: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- review_concat: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tf: vector (nullable = true)\n",
      " |-- tfidf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check schema of output of preprocessing pipeline \n",
    "s_df_transform.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multiclass Models\n",
    "## 3.1. Simple logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9191, 2382]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random split in train and test set with 80-20% proportions\n",
    "training_df, testing_df = s_df.randomSplit([0.8, 0.2], seed=42)\n",
    "[training_df.count(), testing_df.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add logistic regression to the previously defined pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression().\\\n",
    "    setLabelCol('review_score').\\\n",
    "    setFeaturesCol('tfidf').\\\n",
    "    setRegParam(0.0).\\\n",
    "    setMaxIter(100).\\\n",
    "    setElasticNetParam(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new pipeline to chain idf_pipeline with logistic regression\n",
    "# fit training set on pipeline\n",
    "lr_pipeline = Pipeline(stages=[tfidf_pipeline, lr]).fit(training_df)\n",
    "\n",
    "# precict on test and calculate accuracy\n",
    "lr_predictions = lr_pipeline.transform(testing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score above seems OK but now let's check the accuracy per class. we see it is not good for all but 5\n",
    "def printClassPredictions(predictions):\n",
    "    predictions.select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('Score = 1')\n",
    "    predictions.filter(predictions['review_score'] == 1).\\\n",
    "        select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('Score = 2')\n",
    "    predictions.filter(predictions['review_score'] == 2).\\\n",
    "        select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('Score = 3')\n",
    "    predictions.filter(predictions['review_score'] == 3).\\\n",
    "        select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('Score = 4')\n",
    "    predictions.filter(predictions['review_score'] == 4).\\\n",
    "        select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('Score = 5')\n",
    "    predictions.filter(predictions['review_score'] == 5).\\\n",
    "        select(fn.expr('float(prediction = review_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.8303946263643996|\n",
      "+------------------+\n",
      "\n",
      "Score = 1\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.3448275862068966|\n",
      "+------------------+\n",
      "\n",
      "Score = 2\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.20512820512820512|\n",
      "+-------------------+\n",
      "\n",
      "Score = 3\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.25675675675675674|\n",
      "+-------------------+\n",
      "\n",
      "Score = 4\n",
      "+-----------------+\n",
      "|     avg(correct)|\n",
      "+-----------------+\n",
      "|0.582089552238806|\n",
      "+-----------------+\n",
      "\n",
      "Score = 5\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.9165354330708662|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printClassPredictions(lr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Logistic regression with stratified split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size intersect: 0\n",
      "# rows training set: 9191\n",
      "# rows per class\n",
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5| 7454|\n",
      "|           4| 1216|\n",
      "|           3|  270|\n",
      "|           2|  137|\n",
      "|           1|  114|\n",
      "+------------+-----+\n",
      "\n",
      "# rows test set: 2382\n",
      "# rows per class\n",
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5| 1929|\n",
      "|           4|  313|\n",
      "|           3|   76|\n",
      "|           2|   33|\n",
      "|           1|   31|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now make a new stratified split to make sure we have enough representative examples in the train set\n",
    "training_strat_df = s_df.sampleBy(\"review_score\", fractions={1: 0.8, 2: 0.8, 3: 0.8, 4: 0.8, 5: 0.8}, seed=42)\n",
    "test_strat_df = s_df.subtract(training_strat_df)\n",
    "# training set\n",
    "intersect = training_strat_df.select('review_id').intersect(test_strat_df.select('review_id'))\n",
    "print('size intersect: ' + str(intersect.count()))\n",
    "\n",
    "print('# rows training set: ' + str(training_strat_df.count()))\n",
    "print('# rows per class')\n",
    "training_strat_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()\n",
    "# test set\n",
    "print('# rows test set: ' + str(test_strat_df.count()))\n",
    "print('# rows per class')\n",
    "test_strat_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.8119227539882452|\n",
      "+------------------+\n",
      "\n",
      "Score = 1\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         0.0|\n",
      "+------------+\n",
      "\n",
      "Score = 2\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         0.0|\n",
      "+------------+\n",
      "\n",
      "Score = 3\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         0.0|\n",
      "+------------+\n",
      "\n",
      "Score = 4\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.01597444089456869|\n",
      "+-------------------+\n",
      "\n",
      "Score = 5\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         1.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new prevision with previously defined en_lr (elestic net logistic regression)\n",
    "en_lr_strat_pipeline = Pipeline(stages=[tfidf_pipeline, en_lr]).fit(training_strat_df)\n",
    "# fitting + accuracy estimation\n",
    "predictions_en_lr_strat = en_lr_strat_pipeline.transform(test_strat_df)\n",
    "\n",
    "printClassPredictions(predictions_en_lr_strat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Logistic regression with down sampling\n",
    "Sampling down all class to the smallest 1 (= 1 star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows per class\n",
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5| 9383|\n",
      "|           4| 1529|\n",
      "|           3|  346|\n",
      "|           2|  170|\n",
      "|           1|  145|\n",
      "+------------+-----+\n",
      "\n",
      "Total # rows in data set: 11573\n"
     ]
    }
   ],
   "source": [
    "# recap data set size and distribution\n",
    "print('# rows per class')\n",
    "s_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()\n",
    "print('Total # rows in data set: '+ str(s_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           3|  162|\n",
      "|           2|  158|\n",
      "|           4|  154|\n",
      "|           5|  147|\n",
      "|           1|  145|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# downsampling to 145 rows per class (# rows for class 1)\n",
    "downsampled_data = s_df.sampleBy('review_score',\n",
    "    fractions={1: 1, 2: 145./170, 3: 145./346, 4: 145./1529, 5: 145./9383}) \\\n",
    "    .cache()\n",
    "\n",
    "downsampled_data.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[605, 161]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random split in train and test set with 80-20% proportions\n",
    "training_down_df, testing_down_df = downsampled_data.randomSplit([0.8, 0.2], seed=42)\n",
    "[training_down_df.count(), testing_down_df.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.40372670807453415|\n",
      "+-------------------+\n",
      "\n",
      "Score = 1\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.4857142857142857|\n",
      "+------------------+\n",
      "\n",
      "Score = 2\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|       0.375|\n",
      "+------------+\n",
      "\n",
      "Score = 3\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.3870967741935484|\n",
      "+------------------+\n",
      "\n",
      "Score = 4\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.5172413793103449|\n",
      "+------------------+\n",
      "\n",
      "Score = 5\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.2647058823529412|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new prevision with previously defined en_lr (elestic net logistic regression)\n",
    "en_lr_down_pipeline = Pipeline(stages=[tfidf_pipeline, en_lr]).fit(training_down_df)\n",
    "# fitting + accuracy estimation\n",
    "predictions_en_lr_down = en_lr_down_pipeline.transform(testing_down_df)\n",
    "\n",
    "printClassPredictions(predictions_en_lr_down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Logistic regression with up and down sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set before up - down sampling: 9509\n",
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5| 7724|\n",
      "|           4| 1254|\n",
      "|           3|  274|\n",
      "|           2|  138|\n",
      "|           1|  119|\n",
      "+------------+-----+\n",
      "\n",
      "test set: 2064\n",
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5| 1659|\n",
      "|           4|  275|\n",
      "|           3|   72|\n",
      "|           2|   32|\n",
      "|           1|   26|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using down - up sampling to build a train set with 2000 example per class\n",
    "# Test set = 20% of train set = 10000 * 0.2 = 1500 rows\n",
    "# Attention: test set cannot contains exmaple fro test set and must be built before up-down sampling\n",
    "#\n",
    "# Get test set of 2000 rows with a factor 'prop'\n",
    "prop = (11500.-2000.)/11500\n",
    "training_updown_df_pre = s_df.sampleBy(\"review_score\", fractions={1: prop, 2: prop, 3: prop, 4: prop, 5: prop}, seed=42)\n",
    "test_updown_df = s_df.subtract(training_updown_df_pre)\n",
    "# training set before up - down sampling\n",
    "print('training set before up - down sampling: ' + str(training_updown_df_pre.count()))\n",
    "training_updown_df_pre.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()\n",
    "# test set\n",
    "print('test set: ' + str(test_updown_df.count()))\n",
    "test_updown_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows in the train set\n",
      "Total: 9693\n",
      "Per class:\n",
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           4| 1997|\n",
      "|           1| 1993|\n",
      "|           2| 1936|\n",
      "|           5| 1918|\n",
      "|           3| 1849|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# perform up and down sampling on the trainig set so that each class contains +- 1500 rows\n",
    "df_class_1 = training_updown_df_pre[training_updown_df_pre['review_score'] == 1]\n",
    "df_class_2 = training_updown_df_pre[training_updown_df_pre['review_score'] == 2]\n",
    "df_class_3 = training_updown_df_pre[training_updown_df_pre['review_score'] == 3]\n",
    "df_class_4 = training_updown_df_pre[training_updown_df_pre['review_score'] == 4]\n",
    "df_class_5 = training_updown_df_pre[training_updown_df_pre['review_score'] == 5]\n",
    "\n",
    "df_class_1_over = df_class_1.sample(withReplacement=True, fraction=2000./117, seed = 42)\n",
    "df_class_2_over = df_class_2.sample(withReplacement=True, fraction=2000./139, seed = 42)\n",
    "df_class_3_over = df_class_3.sample(withReplacement=True, fraction=2000./290, seed = 42)\n",
    "df_class_4_over = df_class_4.sample(withReplacement=True, fraction=2000./1242, seed = 42)\n",
    "df_class_5_under = df_class_5.sample(withReplacement=True, fraction=2000./7721, seed = 42)\n",
    "\n",
    "import functools \n",
    "\n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1,df2: df1.union(df2.select(df1.columns)), dfs) \n",
    "\n",
    "training_updown_df = unionAll([df_class_1_over, df_class_2_over, df_class_3_over, df_class_4_over, df_class_5_under])\n",
    "\n",
    "print('# of rows in the train set')\n",
    "print('Total: ' + str(training_updown_df.count()))\n",
    "print('Per class:')\n",
    "training_updown_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.7257751937984496|\n",
      "+------------------+\n",
      "\n",
      "Score = 1\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.11538461538461539|\n",
      "+-------------------+\n",
      "\n",
      "Score = 2\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|     0.34375|\n",
      "+------------+\n",
      "\n",
      "Score = 3\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.2638888888888889|\n",
      "+------------------+\n",
      "\n",
      "Score = 4\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.6727272727272727|\n",
      "+------------------+\n",
      "\n",
      "Score = 5\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.7715491259795058|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new prevision with previously defined en_lr (elestic net logistic regression)\n",
    "lr_updown_pipeline = Pipeline(stages=[tfidf_pipeline, lr]).fit(training_updown_df)\n",
    "# fitting + accuracy estimation\n",
    "predictions_lr_updown = lr_updown_pipeline.transform(test_updown_df)\n",
    "\n",
    "printClassPredictions(predictions_lr_updown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.04624088782504624|\n",
      "+-------------------+\n",
      "\n",
      "Score = 1\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         0.0|\n",
      "+------------+\n",
      "\n",
      "Score = 2\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.07633587786259542|\n",
      "+-------------------+\n",
      "\n",
      "Score = 3\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.10294117647058823|\n",
      "+-------------------+\n",
      "\n",
      "Score = 4\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.3241206030150754|\n",
      "+------------------+\n",
      "\n",
      "Score = 5\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         0.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, Tokenizer, CountVectorizer, IDF, IndexToString, HashingTF, StopWordsRemover\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# import nltk stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "# 3. Filter out stop words\n",
    "sw_filter = StopWordsRemover()\\\n",
    "    .setStopWords(stop_words)\\\n",
    "    .setCaseSensitive(False)\\\n",
    "    .setInputCol(\"words\")\\\n",
    "    .setOutputCol(\"filtered\")\n",
    "\n",
    "s_df = s_df.withColumn('label',fn.col('review_score'))\n",
    "\n",
    "train_nb, test_nb = s_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "#train_nb = s_df.sampleBy(\"review_score\", fractions={1: 0.8, 2: 0.8, 3: 0.8, 4: 0.8, 5: 0.8}, seed=42)\n",
    "#test_nb = s_df.subtract(train_nb)\n",
    "\n",
    "#categoryIndexer_nb = StringIndexer(inputCol=\"review_score\", outputCol=\"label\", handleInvalid = \"keep\")\n",
    "tokenizer_nb = Tokenizer(inputCol=\"review_concat\", outputCol=\"words\")\n",
    "hashingTF_nb = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=10000)\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "#categoryConverter_nb = IndexToString(inputCol=\"prediction\", outputCol=\"predCategory\")\n",
    "\n",
    "#nb_pipeline = Pipeline(stages=[categoryIndexer_nb, tokenizer_nb, sw_filter, hashingTF_nb, nb, categoryConverter_nb])\n",
    "nb_pipeline = Pipeline(stages=[tokenizer_nb, sw_filter, hashingTF_nb, nb])\n",
    "\n",
    "nb_model = nb_pipeline.fit(train_nb)\n",
    "nb_predictions = nb_model.transform(train_nb)\n",
    "printClassPredictions(nb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|review_score|label|prediction|\n",
      "+------------+-----+----------+\n",
      "|           3|    3|       4.0|\n",
      "|           5|    5|       4.0|\n",
      "|           5|    5|       4.0|\n",
      "|           5|    5|       4.0|\n",
      "|           5|    5|       4.0|\n",
      "|           5|    5|       4.0|\n",
      "|           5|    5|       3.0|\n",
      "|           5|    5|       4.0|\n",
      "|           5|    5|       4.0|\n",
      "|           5|    5|       4.0|\n",
      "|           4|    4|       3.0|\n",
      "|           5|    5|       3.0|\n",
      "|           5|    5|       4.0|\n",
      "|           5|    5|       4.0|\n",
      "|           5|    5|       4.0|\n",
      "|           5|    5|       4.0|\n",
      "|           5|    5|       4.0|\n",
      "|           5|    5|       4.0|\n",
      "|           5|    5|       4.0|\n",
      "|           4|    4|       3.0|\n",
      "+------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_predictions.select('review_score', 'label', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------+--------------------+\n",
      "|prediction|label|review_score|       review_concat|\n",
      "+----------+-----+------------+--------------------+\n",
      "|       0.0|  0.0|           5|Go Be Kind This b...|\n",
      "|       0.0|  0.0|           5|amazing book grea...|\n",
      "|       0.0|  0.0|           5|Such a blessing t...|\n",
      "|       0.0|  0.0|           5|Page-turner. Terr...|\n",
      "|       0.0|  0.0|           5|Elegant with just...|\n",
      "|       0.0|  0.0|           5|My heart is still...|\n",
      "|       0.0|  0.0|           5|Hard going I gave...|\n",
      "|       0.0|  0.0|           5|Hilarious and ins...|\n",
      "|       0.0|  0.0|           5|757 pages of jour...|\n",
      "|       0.0|  0.0|           5|Totally engrossin...|\n",
      "|       0.0|  0.0|           5|The COUNT One of ...|\n",
      "|       0.0|  0.0|           5|Difficult to cont...|\n",
      "|       0.0|  0.0|           5|Funny, serious, i...|\n",
      "|       0.0|  0.0|           5|This book has cha...|\n",
      "|       0.0|  0.0|           5|Best book I've ev...|\n",
      "|       0.0|  0.0|           5|Great Read Well w...|\n",
      "|       0.0|  0.0|           5|Best writing on t...|\n",
      "|       0.0|  0.0|           5|Five Stars Nice r...|\n",
      "|       0.0|  0.0|           5|This really is a ...|\n",
      "|       0.0|  0.0|           5|Very Useful Very ...|\n",
      "+----------+-----+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_predictions.filter(nb_predictions['review_score'] == 5).\\\n",
    "        select('prediction', 'label', 'review_score', 'review_concat').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "predictions = rfModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"review_score\", outputCol=\"indexedLabel\").fit(s_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = s_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "assembler = VectorAssembler(inputCols=['tfidf','sentimentIndex'],outputCol=\"indexedFeatures\")\n",
    "\n",
    "# Train a GBT model.\n",
    "rf = RandomForestClassifier(labelCol=\"review_score\", featuresCol=\"indexedFeatures\", numTrees=1)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[tfidf_pipeline, assembler, rf])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"review_score\", \"indexedFeatures\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"review_score\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "gbtModel = model.stages[2]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Binary Models\n",
    "1 to 3 stars = 0 and 4 / 5 stars = 1\n",
    "## 4.1 Logistic regression\n",
    "with hyper parameter tuning on dedicated validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with easy implemetation: only consider the content of the 2 fields review_title and review_text\n",
    "# concantenate them in one new field \"review_concat\"from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "filepath = 'data_processed/ExctractedData.json'\n",
    "# load JSON file\n",
    "s_df = spark.read.json(filepath)\n",
    "s_df.count()\n",
    "s_df = s_df.drop_duplicates(subset=['review_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- book_title: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- review_score: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- review_title: string (nullable = true)\n",
      " |-- review_user: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- review_concat: string (nullable = true)\n",
      " |-- bin_score: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# concatenate review text and title in one field\n",
    "s_df = s_df.withColumn('review_concat',fn.concat(fn.col('review_title'),fn.lit(' '), fn.col('review_text')))\n",
    "# review_score is of type String ==> cast it from String to Integer\n",
    "s_df = s_df.withColumn(\"review_score\", s_df[\"review_score\"].cast(IntegerType()))\n",
    "\n",
    "# add new binary score (0 or 1), \n",
    "# 1 to 3 stars = 0 and 4 to 5 stars = 1\n",
    "from pyspark.sql.functions import udf\n",
    "def scoreToBin(value):\n",
    "   if   value < 4: return 0\n",
    "   else : return 1\n",
    "udfScoreToBin = udf(scoreToBin, IntegerType())\n",
    "s_df = s_df.withColumn(\"bin_score\", udfScoreToBin(\"review_score\"))\n",
    "s_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of rows: 11573\n",
      "# of rows per review score:\n",
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5| 9383|\n",
      "|           4| 1529|\n",
      "|           3|  346|\n",
      "|           2|  170|\n",
      "|           1|  145|\n",
      "+------------+-----+\n",
      "\n",
      "# of rows per BINARY review score:\n",
      "+---------+-----+\n",
      "|bin_score|count|\n",
      "+---------+-----+\n",
      "|        1|10912|\n",
      "|        0|  661|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Total # of rows: ' + str(s_df.count()))\n",
    "print('# of rows per review score:')\n",
    "s_df.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()\n",
    "print('# of rows per BINARY review score:')\n",
    "s_df.groupBy(\"bin_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows training set: 8027\n",
      "# rows per class\n",
      "+---------+-----+\n",
      "|bin_score|count|\n",
      "+---------+-----+\n",
      "|        1| 7579|\n",
      "|        0|  448|\n",
      "+---------+-----+\n",
      "\n",
      "# rows validation set: 1105\n",
      "# rows per class\n",
      "+---------+-----+\n",
      "|bin_score|count|\n",
      "+---------+-----+\n",
      "|        1| 1031|\n",
      "|        0|   74|\n",
      "+---------+-----+\n",
      "\n",
      "# rows test set: 2441\n",
      "# rows per class\n",
      "+---------+-----+\n",
      "|bin_score|count|\n",
      "+---------+-----+\n",
      "|        1| 2302|\n",
      "|        0|  139|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now make a new stratified split 70-10-20% with same proportion of bin_score 0 and 1\n",
    "training_strat_df = s_df.sampleBy(\"bin_score\", fractions={0: 0.7, 1: 0.7}, seed=42)\n",
    "test_valid_strat_df = s_df.subtract(training_strat_df)\n",
    "\n",
    "valid_strat_df = test_valid_strat_df.sampleBy(\"bin_score\", fractions={0: 0.33, 1: 0.33}, seed=42)\n",
    "test_strat_df = test_valid_strat_df.subtract(valid_strat_df)\n",
    "\n",
    "# show some stats\n",
    "# training sets\n",
    "print('# rows training set: ' + str(training_strat_df.count()))\n",
    "print('# rows per class')\n",
    "training_strat_df.groupBy(\"bin_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()\n",
    "# validation set\n",
    "print('# rows validation set: ' + str(valid_strat_df.count()))\n",
    "print('# rows per class')\n",
    "valid_strat_df.groupBy(\"bin_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()\n",
    "# test set\n",
    "print('# rows test set: ' + str(test_strat_df.count()))\n",
    "print('# rows per class')\n",
    "test_strat_df.groupBy(\"bin_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows in the train set\n",
      "Total: 9747\n",
      "Per class:\n",
      "+---------+-----+\n",
      "|bin_score|count|\n",
      "+---------+-----+\n",
      "|        1| 7579|\n",
      "|        0| 2168|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# perform up sampling on the trainig to increase the number of reviews with bin_score = 0\n",
    "# increase with a factor 5 to get above 2000 reviews with bin_score = 0\n",
    "df_class_0 = training_strat_df[training_strat_df['bin_score'] == 0]\n",
    "df_class_0_over = df_class_0.sample(withReplacement=True, fraction=5., seed = 42)\n",
    "\n",
    "df_class_1 = training_strat_df[training_strat_df['bin_score'] == 1]\n",
    "\n",
    "import functools \n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1,df2: df1.union(df2.select(df1.columns)), dfs) \n",
    "\n",
    "training_up_df = unionAll([df_class_0_over, df_class_1])\n",
    "\n",
    "print('# of rows in the train set')\n",
    "print('Total: ' + str(training_up_df.count()))\n",
    "print('Per class:')\n",
    "training_up_df.groupBy(\"bin_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# define pre-processing and classification pipeline\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import IDF, RegexTokenizer, StringIndexer, StopWordsRemover, CountVectorizer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# import nltk stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "# 1. String indexer: convert book_id (string) to unique numeric undex\n",
    "book_stringIdx = StringIndexer() \\\n",
    "    .setHandleInvalid(\"keep\")\\\n",
    "    .setInputCol(\"book_id\")\\\n",
    "    .setOutputCol(\"book_label\")\n",
    "\n",
    "# 2. Tokenizer, .setPattern(\"\\\\p{L}+\") means that it remove accent from words\n",
    "regex_tokenizer = RegexTokenizer()\\\n",
    "    .setGaps(False)\\\n",
    "    .setPattern(\"\\\\p{L}+\")\\\n",
    "    .setInputCol(\"review_concat\")\\\n",
    "    .setOutputCol(\"words\")\n",
    "\n",
    "# 3. Filter out stop words\n",
    "stopword_remover = StopWordsRemover()\\\n",
    "    .setStopWords(stop_words)\\\n",
    "    .setCaseSensitive(False)\\\n",
    "    .setInputCol(\"words\")\\\n",
    "    .setOutputCol(\"filtered\")\n",
    "\n",
    "# 4. TF: TF vectorization + remove words that appear in 5 docs or less\n",
    "# converts text documents to vectors of term counts\n",
    "count_vectorizer = CountVectorizer(minDF=5)\\\n",
    "    .setInputCol(\"filtered\")\\\n",
    "    .setOutputCol(\"tf\")\n",
    "\n",
    "# 5. TF-IDF transform\n",
    "# The IDFModel takes feature vectors (generally created from HashingTF or CountVectorizer) and scales each column. \n",
    "# Intuitively, it down-weights columns which appear frequently in a corpus.\n",
    "idf = IDF()\\\n",
    "    .setInputCol(\"tf\")\\\n",
    "    .setOutputCol(\"tfidf\")\n",
    "\n",
    "# 6. Feature assembler\n",
    "# assemble tfidf tectual features with book_label\n",
    "assembler = VectorAssembler(inputCols=['tfidf','book_label'],outputCol=\"tfidf_book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to calculate and print prediction results\n",
    "def printClassPredictions_bin(predictions):\n",
    "    predictions.select(fn.expr('float(prediction = bin_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('bin_score = 0')\n",
    "    predictions.filter(predictions['bin_score'] == 0).\\\n",
    "        select(fn.expr('float(prediction = bin_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()\n",
    "    print('bin_score = 1')\n",
    "    predictions.filter(predictions['bin_score'] == 1).\\\n",
    "        select(fn.expr('float(prediction = bin_score)').alias('correct')).\\\n",
    "        select(fn.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.9457013574660633|\n",
      "+------------------+\n",
      "\n",
      "bin_score = 0\n",
      "+-------------------+\n",
      "|       avg(correct)|\n",
      "+-------------------+\n",
      "|0.43243243243243246|\n",
      "+-------------------+\n",
      "\n",
      "bin_score = 1\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.9825412221144519|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(featuresCol=assembler.getOutputCol(), labelCol=\"bin_score\")\n",
    "\n",
    "pipeline = Pipeline(stages=[book_stringIdx, regex_tokenizer, stopword_remover,\n",
    "    count_vectorizer, idf, assembler, lr])\n",
    "\n",
    "model = pipeline.fit(training_up_df)\n",
    "predictionsBin = model.transform(valid_strat_df)\n",
    "printClassPredictions_bin(predictionsBin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.9330316742081448|\n",
      "+------------------+\n",
      "\n",
      "bin_score = 0\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         0.0|\n",
      "+------------+\n",
      "\n",
      "bin_score = 1\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         1.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_en1 = LogisticRegression(featuresCol=assembler.getOutputCol(), labelCol=\"bin_score\",\n",
    "        regParam = 0.1, elasticNetParam = 0.8)\n",
    "\n",
    "pipeline_en1 = Pipeline(stages=[book_stringIdx, regex_tokenizer, stopword_remover,\n",
    "    count_vectorizer, idf, assembler, lr_en1])\n",
    "\n",
    "model_en1 = pipeline_en1.fit(training_up_df)\n",
    "predictionsBin_en1 = model_en1.transform(valid_strat_df)\n",
    "printClassPredictions_bin(predictionsBin_en1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.9312217194570136|\n",
      "+------------------+\n",
      "\n",
      "bin_score = 0\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         0.0|\n",
      "+------------+\n",
      "\n",
      "bin_score = 1\n",
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.9980601357904947|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_en2 = LogisticRegression(featuresCol=assembler.getOutputCol(), labelCol=\"bin_score\",\n",
    "        regParam = 0.1, elasticNetParam = 0.5)\n",
    "\n",
    "pipeline_en2 = Pipeline(stages=[book_stringIdx, regex_tokenizer, stopword_remover,\n",
    "    count_vectorizer, idf, assembler, lr_en2])\n",
    "\n",
    "model_en2 = pipeline_en2.fit(training_up_df)\n",
    "predictionsBin_en2 = model_en2.transform(valid_strat_df)\n",
    "printClassPredictions_bin(predictionsBin_en2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.9330316742081448|\n",
      "+------------------+\n",
      "\n",
      "bin_score = 0\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         0.0|\n",
      "+------------+\n",
      "\n",
      "bin_score = 1\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         1.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_en3 = LogisticRegression(featuresCol=assembler.getOutputCol(), labelCol=\"bin_score\",\n",
    "        regParam = 0.3, elasticNetParam = 0.8)\n",
    "\n",
    "pipeline_en3 = Pipeline(stages=[book_stringIdx, regex_tokenizer, stopword_remover,\n",
    "    count_vectorizer, idf, assembler, lr_en3])\n",
    "\n",
    "model_en3 = pipeline_en3.fit(training_up_df)\n",
    "predictionsBin_en3 = model_en3.transform(valid_strat_df)\n",
    "printClassPredictions_bin(predictionsBin_en3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.9330316742081448|\n",
      "+------------------+\n",
      "\n",
      "bin_score = 0\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         0.0|\n",
      "+------------+\n",
      "\n",
      "bin_score = 1\n",
      "+------------+\n",
      "|avg(correct)|\n",
      "+------------+\n",
      "|         1.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_en4 = LogisticRegression(featuresCol=assembler.getOutputCol(), labelCol=\"bin_score\",\n",
    "        regParam = 0.3, elasticNetParam = 0.5)\n",
    "\n",
    "pipeline_en4 = Pipeline(stages=[book_stringIdx, regex_tokenizer, stopword_remover,\n",
    "    count_vectorizer, idf, assembler, lr_en4])\n",
    "\n",
    "model_en4 = pipeline_en4.fit(training_up_df)\n",
    "predictionsBin_en4 = model_en4.transform(valid_strat_df)\n",
    "printClassPredictions_bin(predictionsBin_en4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluating best model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsBin_test = model.transform(test_strat_df)\n",
    "printClassPredictions_binprintClassPredictions(predictionsBin_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
